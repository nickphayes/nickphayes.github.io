---
layout: post
title: "The Landscape of Responsible AI"
categories: "AI"
featImg: neuralnetwork.png
excerpt: "Defining terms to better understand the field"
permalink: "landscape-of-responsible-ai"
style: 
---

---
### Introduction
This post surveys the field of Responsible AI, clarifying terminology and providing context and motivations for subfields. 
It is not a typology, nor a source of exacting definitions; rather, it is an overview of the key concepts and research agendas
popular at the time of writing 

[last update: 12/2024]. 

---

**AI**: Artificial Intelligence (AI) is bit of a nebulous term, as it has taken on many meanings since its original conception in the 1950s. In general, AI refers to the ability of a machine to perform a task typically reserved for humans. Its modern usage usually refers more explicitly to machine learning and/or reinforcement learning, with an emphasis on the capacity to learn and solve complex problems. 

**Foundational Models**: Foundational models are generally large, multimodal, general-purpose models trained on massive datasets. Some pieces of legislation use a compute threshold to categorize a model as "foundational" or not, but there is not consensus on how big a model must be for it to be a considered foundational. Because of their scale, only a handful of companies are in the business of developing foundational models. Until relatively recently, most work in AI could be distinctly categorized as speech/language processing, computer vision, or robotics. These fields arguably emulate the main ways in which humans interact with the world--through sight (computer vision), sound (speech/language processing), and physical touch (robotics)[^a]. Crucially, early models in these fields were quite small and specialized in comparison to today's models. 

**AGI**: Artifical general intelligence (AGI) is, once again, a bit of a nebulous term. Usually, it describes an AI system that performs at the level of human intelligence across all (or most) tasks. In some domains, we have arguably achieved or exceeded human level intelligence (e.g. translating languages), but full-scale, general-purpose AGI has yet to be developed. 

**Superintelligence**: Superintelligence is the next step up from AGI, describing an AI system that outperforms human intelligence. AGI/Superintelligent AI are the main source of apprehension regarding catastrophic risk from AI. 

**Frontier AI**: Frontier AI refers to future-learning capabilities of the model(s) that are currently the most powerful--i.e. those at the frontier, inching closer and closer to AGI. Accordingly, *frontier safety* focuses on anticipating the new risks introduced by the next generation of model capabilities.   

**AI X-risk/Catastrophic Risk**: Existential risk (X-risk) and catastrophic risk from AI refers to the possibility that AI could cause human extinction or, at the very least, cause large-scale catastrophe. Among other examples, some hypothesize that powerful AI could lead to biological attacks, financial meltdowns, autocratic dictatorships, conflict escalation, and psychological manipulation, threatening the prosperity of the human race at the global level. 

**Trust and Safety**: In line with traditional Trust and Safety teams in the tech-world, AI Trust and Safety focuses on short-term accidents and misuse. Making sure models don't produce harmful content, keeping user information private and secure, staying ahead of threat actors/adversaries--these are the types of issues that fall under the domain of AI Trust and Safety. 

**Responsible AI**: In my view, responsible AI is the umbrella under which many subfields of AI safety, security, ethics, and governance fall. Responsible AI refers to the safe, ethical, and trustworthy development of increasingly capable AI systems. 


**AI Safety**
**Adversarial robustness**
**Alignment**
**Outer alignment**
**Inner alignment**
**Explainability**
**Interpretability**
**Mechanistic Interpetability**
**Evaluations**
**Scalable Oversight**
**Multi-Agent Systems**

**Trustworthy and Secure AI**
**Trustworthy AI**
**Secure AI**
**AI Assurance**

**AI Governance**
**AI Geopolitics**
**AI Ethics**

[^a]: As a fun sidenote, a few researchers have even explored taste/smell as potential sources of inspiration for AI development. 